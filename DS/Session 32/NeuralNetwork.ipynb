{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "class BinaryCrossEntropy:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __call__(self, y_pred, y_true):\r\n",
    "        ix_zeros = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 0]\r\n",
    "        ix_ones = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 1]\r\n",
    "\r\n",
    "        y_zero = np.log(1 - y_pred[ix_zeros] + 1e-10).sum()\r\n",
    "        y_one = np.log(y_pred[ix_ones] + 1e-10).sum()\r\n",
    "\r\n",
    "        return -1 * (y_zero + y_one)\r\n",
    "    \r\n",
    "    def grad_input(self, X, y_true):\r\n",
    "        if y_true == 0:\r\n",
    "            return 1/(1-X)\r\n",
    "        else:\r\n",
    "            return -1/X \r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "class Sigmoid:\r\n",
    "    def __call__(self, X):\r\n",
    "        return self.eval(X)\r\n",
    "    \r\n",
    "    def eval(self, X):\r\n",
    "        return 1/(1+np.e**(-1*X))\r\n",
    "\r\n",
    "    def grad_input(self, X):\r\n",
    "        return np.identity(X.shape[0])*self.eval(X)*(1 - self.eval(X))\r\n",
    "\r\n",
    "class Dot:\r\n",
    "    def __init__(self, input_size, units):\r\n",
    "        self.W = np.random.randn(input_size, units)\r\n",
    "        self.b = np.random.randn(units, 1)\r\n",
    "\r\n",
    "    def __call__(self, X):\r\n",
    "        return self.W.T.dot(X) + self.b\r\n",
    "\r\n",
    "    def grad_w(self, X):\r\n",
    "        I = np.identity(self.b.shape[0])\r\n",
    "        grad = np.stack([I]*self.W.shape[0], axis=1)*X\r\n",
    "        return np.transpose(grad, [1, 0, 2])\r\n",
    "    \r\n",
    "    def grad_b(self):\r\n",
    "        return np.identity(self.b.shape[0])\r\n",
    "\r\n",
    "    def grad_input(self):\r\n",
    "        return self.W.T\r\n",
    "    \r\n",
    "    def get_output_size(self):\r\n",
    "        return self.b.shape\r\n",
    "    \r\n",
    "    def get_no_of_params(self):\r\n",
    "        return np.prod(self.W.shape) + np.prod(self.b.shape)\r\n",
    "    \r\n",
    "    def update(self, gradW, gradb, optimizer, method):\r\n",
    "        if method == \"minimize\":\r\n",
    "            self.W = optimizer.minimize(self.W, gradW)\r\n",
    "            self.b = optimizer.minimize(self.b, gradb)\r\n",
    "        elif method == \"maximize\":\r\n",
    "            self.W = optimizer.maximize(self.W, gradW)\r\n",
    "            self.b = optimizer.maximize(self.b, gradb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "class Dense:\r\n",
    "    \r\n",
    "    def __init__(self, units, activation, input_size):\r\n",
    "        self.units = units\r\n",
    "        self.dot = Dot(input_size, units)\r\n",
    "        self.activation = activation\r\n",
    "        self.input_size = input_size\r\n",
    "\r\n",
    "    def get_output_size(self):\r\n",
    "        return self.dot.get_output_size()\r\n",
    "\r\n",
    "    def get_no_of_params(self):\r\n",
    "        return self.dot.get_no_of_params()\r\n",
    "\r\n",
    "    def eval(self, X):\r\n",
    "        return self.activation(self.dot(X))\r\n",
    "\r\n",
    "    def grad_parameters(self, X):\r\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\r\n",
    "        dI_dw = self.dot.grad_w(X)\r\n",
    "        da_dw = da_dI.dot(dI_dw)\r\n",
    "        dI_db = self.dot.grad_b()\r\n",
    "        da_db = da_dI.dot(dI_db)\r\n",
    "        return (np.transpose(da_dw, [1,0,2]), da_db)\r\n",
    "    \r\n",
    "    def grad_input(self, X):\r\n",
    "        g1 = self.activation.grad_input(self.dot(X))\r\n",
    "\r\n",
    "        g2 = self.dot.grad_input()\r\n",
    "\r\n",
    "        return g1.dot(g2)\r\n",
    "    \r\n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\r\n",
    "        self.dot.update(grad_w, grad_b, optimizer, method)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "class GradientDescentOptimizer:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def set_lr(self, learning_rate):\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        return self\r\n",
    "    \r\n",
    "    def minimize(self, X, grad_X):\r\n",
    "        assert X.shape == grad_X.shape, f\"Shape mismatch, Input shape {X.shape} != Gradient shape {grad_X.shape}\"\r\n",
    "        return X - (self.learning_rate*grad_X)\r\n",
    "    \r\n",
    "    def maximize(self, X, grad_X):\r\n",
    "        assert X.shape == grad_X.shape, f\"Shape mismatch, Input shape {X.shape} != Gradient shape {grad_X.shape}\"\r\n",
    "        return X + (self.learning_rate*grad_X)\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "class Sequential:\r\n",
    "    def __init__(self):\r\n",
    "        self.layers = []\r\n",
    "        self.loss = None\r\n",
    "        self.outputs = []\r\n",
    "\r\n",
    "    def add(self, layer):\r\n",
    "        self.layers.append(layer)\r\n",
    "        return self\r\n",
    "    \r\n",
    "    def summary(self):\r\n",
    "        from tabulate import tabulate\r\n",
    "\r\n",
    "        headers = [\"Layer Type\", \"Output Shape\", \"No. of parameters\"]\r\n",
    "        summary_ = []\r\n",
    "        params = 0\r\n",
    "        for layer in self.layers:\r\n",
    "            p = layer.get_no_of_params()\r\n",
    "            params += p\r\n",
    "            summary_.append([layer.__class__.__name__, layer.get_output_size(), p])\r\n",
    "        \r\n",
    "        print(tabulate(summary_, headers=headers))\r\n",
    "        print(\"Total No. of parameters:\", params)\r\n",
    "    \r\n",
    "    def fit(self, X, y, n_epochs, learning_rate, optimizer, batch_size=1, verbose=1):\r\n",
    "        self.optimizer = optimizer.set_lr(learning_rate)\r\n",
    "        for i in range(n_epochs):\r\n",
    "            if verbose == 1:\r\n",
    "                print(f\"Epoch: {i+1}\") \r\n",
    "            gradients = []\r\n",
    "            for j in range(X.shape[0]):\r\n",
    "                _, outputs, _gradients_ = self.forward_propagation(X[j].reshape(1,-1))\r\n",
    "                grads = self.backward_propagation(outputs, _gradients_, y[j].reshape(1,-1))\r\n",
    "                gradients.append(grads)\r\n",
    "                if (j+1) % batch_size == 0:\r\n",
    "                    self._update_params(gradients)\r\n",
    "                    gradients = []\r\n",
    "                    losses = []\r\n",
    "                    if verbose == 1:\r\n",
    "                        print(f\"\\rLoss:{self._eval_loss(X, y)}\", end=\"\")\r\n",
    "            if verbose == 1:\r\n",
    "                print(\"\")\r\n",
    "            if verbose == 0:\r\n",
    "                print(f\"\\rEpoch: {i+1} Loss:{self._eval_loss(X, y)}\", end=\"\")\r\n",
    "            \r\n",
    "        print(\"\")\r\n",
    "            \r\n",
    "    def forward_propagation(self, X, eval=False):\r\n",
    "        output = X.T\r\n",
    "        outputs = [output]\r\n",
    "        gradients = []\r\n",
    "        for layer in self.layers:\r\n",
    "            if not eval:\r\n",
    "                grad_ = {}\r\n",
    "                grad_[\"input\"] = layer.grad_input(output)\r\n",
    "                grad_[\"w\"], grad_[\"b\"] = layer.grad_parameters(output)\r\n",
    "                gradients.append(grad_)\r\n",
    "            output = layer.eval(output)\r\n",
    "            outputs.append(output)\r\n",
    "\r\n",
    "        return output.T, outputs, gradients\r\n",
    "    \r\n",
    "    def backward_propagation(self, outputs, gradients, y):\r\n",
    "        grad_loss = self.loss.grad_input(outputs[-1], y)\r\n",
    "        outputs = outputs[:-1]\r\n",
    "        grads = []\r\n",
    "        for grad, output in list(zip(gradients, outputs))[::-1]:\r\n",
    "            grad_w, grad_b = grad_loss.dot(grad[\"w\"])[0], grad_loss.dot(grad[\"b\"]).T\r\n",
    "            grads.append((grad_w, grad_b))\r\n",
    "            grad_loss = grad_loss.dot(grad[\"input\"])\r\n",
    "        \r\n",
    "        return grads\r\n",
    "    \r\n",
    "    def _update_params(self, gradients):\r\n",
    "        grads = [[0, 0] for _ in gradients[0]]\r\n",
    "        for grads_ in gradients:\r\n",
    "            for i in range(len(grads_)):\r\n",
    "                grads[i][0] += grads_[i][0]\r\n",
    "                grads[i][1] += grads_[i][1]\r\n",
    "\r\n",
    "        for ((grad_w, grad_b), layer) in zip(grads, self.layers[::-1]):\r\n",
    "            layer.update(grad_w, grad_b, self.optimizer)\r\n",
    "\r\n",
    "    def _eval(self, X):\r\n",
    "        return self.forward_propagation(X, eval=True)[0]\r\n",
    "    \r\n",
    "    def compile(self, loss):\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "    def _eval_loss(self, X, y_true):\r\n",
    "        if self.loss is None:\r\n",
    "            raise RuntimeError(\"Model not compiled\")\r\n",
    "            \r\n",
    "        return self.loss(self._eval(X), y_true)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(units=3, activation=Sigmoid(), input_size=2))\r\n",
    "model.add(Dense(units=2, activation=Sigmoid(), input_size=3))\r\n",
    "model.add(Dense(units=1, activation=Sigmoid(), input_size=2))\r\n",
    "model.compile(BinaryCrossEntropy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Layer Type    Output Shape      No. of parameters\n",
      "------------  --------------  -------------------\n",
      "Dense         (3, 1)                            9\n",
      "Dense         (2, 1)                            8\n",
      "Dense         (1, 1)                            3\n",
      "Total No. of parameters: 20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "X, y = make_gaussian_quantiles(n_samples=200,n_classes=2)\r\n",
    "# print(\"Y_pred\", model._eval(X))\r\n",
    "print(\"Loss\", model._eval_loss(X, y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss 209.69092719233262\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "# model._eval(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "model.fit(X, y, n_epochs=1000, learning_rate=0.05, optimizer=GradientDescentOptimizer(), verbose=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1000 Loss:8.437956608444045\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "((model._eval(X)>0.5) == y.reshape(-1,1)).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "952603312201d9d1df6f1b6eb4a2044a9cb2cee3bbe4f29af3f4f86c434f8702"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}