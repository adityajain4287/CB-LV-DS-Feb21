{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "class BinaryCrossEntropy:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __call__(self, y_pred, y_true):\r\n",
    "        ix_zeros = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 0]\r\n",
    "        ix_ones = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 1]\r\n",
    "\r\n",
    "        y_zero = np.log(1 - y_pred[ix_zeros] + 1e-10).sum()\r\n",
    "        y_one = np.log(y_pred[ix_ones] + 1e-10).sum()\r\n",
    "\r\n",
    "        return -1 * (y_zero + y_one)\r\n",
    "    \r\n",
    "    def grad_input(self, X, y_true):\r\n",
    "        if y_true == 0:\r\n",
    "            return -1/(1-X)\r\n",
    "        else:\r\n",
    "            return -1/X \r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "class Sigmoid:\r\n",
    "    def __call__(self, X):\r\n",
    "        return self.eval(X)\r\n",
    "    \r\n",
    "    def eval(self, X):\r\n",
    "        return 1/(1+np.e**(-1*X))\r\n",
    "\r\n",
    "    def grad_input(self, X):\r\n",
    "        return self.eval(X)*(1 - self.eval(X))\r\n",
    "\r\n",
    "class Dot:\r\n",
    "    def __init__(self, input_size, units):\r\n",
    "        self.W = np.random.randn(input_size, units)\r\n",
    "        self.b = np.random.randn(units, 1)\r\n",
    "\r\n",
    "    def __call__(self, X):\r\n",
    "        return self.W.T.dot(X) + self.b\r\n",
    "\r\n",
    "    def grad_w(self, X):\r\n",
    "        I = np.identity(self.b.shape[0])\r\n",
    "        return np.stack([I]*self.W.shape[1], axis=1)*X\r\n",
    "    \r\n",
    "    def grad_b(self):\r\n",
    "        return np.identity(self.b.shape[0])\r\n",
    "\r\n",
    "    def grad_input(self):\r\n",
    "        return self.W.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "class Dense:\r\n",
    "\r\n",
    "    def __init__(self, units, activation, input_size):\r\n",
    "        self.units = units\r\n",
    "        self.dot = Dot(input_size, units)\r\n",
    "        self.activation = activation\r\n",
    "        \r\n",
    "    def eval(self, X):\r\n",
    "        return self.activation(self.dot(X))\r\n",
    "\r\n",
    "    def grad_parameters(self, X):\r\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\r\n",
    "        dI_dw = self.dot.grad_w(X)\r\n",
    "        da_dw = da_dI * dI_dw\r\n",
    "        dI_db = self.dot.grad_b(X)\r\n",
    "        da_db = da_dI * dI_db\r\n",
    "        return (da_dw, da_db)\r\n",
    "    \r\n",
    "    def grad_input(self, X):\r\n",
    "        g1 = self.activation.grad_input(self.dot(X))\r\n",
    "\r\n",
    "        g2 = self.dot.grad_input()\r\n",
    "\r\n",
    "        return g1.dot(g2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "class Sequential:\r\n",
    "    def __init__(self):\r\n",
    "        self.layers = []\r\n",
    "        self.loss = None\r\n",
    "        self.outputs = []\r\n",
    "\r\n",
    "    def add(self, layer):\r\n",
    "        self.layers.append(layer)\r\n",
    "        return self\r\n",
    "    \r\n",
    "    def forward_propagation(self, X):\r\n",
    "        output = X.T\r\n",
    "        for layer in self.layers:\r\n",
    "            output = layer.eval(output)\r\n",
    "        \r\n",
    "        return output.T\r\n",
    "\r\n",
    "    def _eval(self, X):\r\n",
    "        return self.forward_propagation(X)\r\n",
    "    \r\n",
    "    def compile(self, loss):\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "    def _eval_loss(self, X, y_true):\r\n",
    "        if self.loss is None:\r\n",
    "            raise RuntimeError(\"Model not compiled\")\r\n",
    "            \r\n",
    "        return self.loss(self._eval(X), y_true)\r\n",
    "    \r\n",
    "    def backward_propagation(self):\r\n",
    "        raise NotImplementedError(\"Backpropagation is not defined!\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(units=2, activation=Sigmoid(), input_size=2))\r\n",
    "model.add(Dense(units=2, activation=Sigmoid(), input_size=2))\r\n",
    "model.add(Dense(units=1, activation=Sigmoid(), input_size=2))\r\n",
    "model.compile(BinaryCrossEntropy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "X = np.random.randn(1, 2)\r\n",
    "print(\"Y_pred\", model._eval(X))\r\n",
    "print(\"Loss\", model._eval_loss(X, np.array([1])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Y_pred [[0.72188274]]\n",
      "Loss 0.3258925695278604\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "952603312201d9d1df6f1b6eb4a2044a9cb2cee3bbe4f29af3f4f86c434f8702"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}