{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "class BinaryCrossEntropy:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __call__(self, y_pred, y_true):\r\n",
    "        ix_zeros = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 0]\r\n",
    "        ix_ones = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 1]\r\n",
    "\r\n",
    "        y_zero = np.log(1 - y_pred[ix_zeros] + 1e-10).sum()\r\n",
    "        y_one = np.log(y_pred[ix_ones] + 1e-10).sum()\r\n",
    "\r\n",
    "        return -1 * (y_zero + y_one)\r\n",
    "    \r\n",
    "    def grad_input(self, X, y_true):\r\n",
    "        if y_true == 0:\r\n",
    "            return 1/(1-X)\r\n",
    "        else:\r\n",
    "            return -1/X \r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "class Sigmoid:\r\n",
    "    def __call__(self, X):\r\n",
    "        return self.eval(X)\r\n",
    "    \r\n",
    "    def eval(self, X):\r\n",
    "        return 1/(1+np.e**(-1*X))\r\n",
    "\r\n",
    "    def grad_input(self, X):\r\n",
    "        return np.identity(X.shape[0])*self.eval(X)*(1 - self.eval(X))\r\n",
    "\r\n",
    "class Dot:\r\n",
    "    def __init__(self, input_size, units):\r\n",
    "        self.W = np.random.randn(input_size, units)\r\n",
    "        self.b = np.random.randn(units, 1)\r\n",
    "\r\n",
    "    def __call__(self, X):\r\n",
    "        return self.W.T.dot(X) + self.b\r\n",
    "\r\n",
    "    def grad_w(self, X):\r\n",
    "        I = np.identity(self.b.shape[0])\r\n",
    "        grad = np.stack([I]*self.W.shape[0], axis=1)*X\r\n",
    "        return np.transpose(grad, [1, 0, 2])\r\n",
    "    \r\n",
    "    def grad_b(self):\r\n",
    "        return np.identity(self.b.shape[0])\r\n",
    "\r\n",
    "    def grad_input(self):\r\n",
    "        return self.W.T\r\n",
    "    \r\n",
    "    def get_output_size(self):\r\n",
    "        return self.b.shape\r\n",
    "    \r\n",
    "    def get_no_of_params(self):\r\n",
    "        return np.prod(self.W.shape) + np.prod(self.b.shape)\r\n",
    "    \r\n",
    "    def update(self, gradW, gradb, optimizer, method):\r\n",
    "        if method == \"minimize\":\r\n",
    "            self.W = optimizer.minimize(self.W, gradW)\r\n",
    "            self.b = optimizer.minimize(self.b, gradb)\r\n",
    "        elif method == \"maximize\":\r\n",
    "            self.W = optimizer.maximize(self.W, gradW)\r\n",
    "            self.b = optimizer.maximize(self.b, gradb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "class Dense:\r\n",
    "\r\n",
    "    def __init__(self, units, activation, input_size):\r\n",
    "        self.units = units\r\n",
    "        self.dot = Dot(input_size, units)\r\n",
    "        self.activation = activation\r\n",
    "        self.input_size = input_size\r\n",
    "\r\n",
    "    def get_output_size(self):\r\n",
    "        return self.dot.get_output_size()\r\n",
    "\r\n",
    "    def get_no_of_params(self):\r\n",
    "        return self.dot.get_no_of_params()\r\n",
    "\r\n",
    "    def eval(self, X):\r\n",
    "        return self.activation(self.dot(X))\r\n",
    "\r\n",
    "    def grad_parameters(self, X):\r\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\r\n",
    "        dI_dw = self.dot.grad_w(X)\r\n",
    "        da_dw = da_dI * dI_dw\r\n",
    "        dI_db = self.dot.grad_b()\r\n",
    "        da_db = da_dI * dI_db\r\n",
    "        return (da_dw, da_db)\r\n",
    "    \r\n",
    "    def grad_input(self, X):\r\n",
    "        g1 = self.activation.grad_input(self.dot(X))\r\n",
    "\r\n",
    "        g2 = self.dot.grad_input()\r\n",
    "\r\n",
    "        return g1.dot(g2)\r\n",
    "    \r\n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\r\n",
    "        self.dot.update(grad_w, grad_b, optimizer, method)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "class GradientDescentOptimizer:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def set_lr(self, learning_rate):\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        return self\r\n",
    "    \r\n",
    "    def minimize(self, X, grad_X):\r\n",
    "        assert X.shape == grad_X.shape, f\"Shape mismatch, Input shape {X.shape} != Gradient shape {grad_X.shape}\"\r\n",
    "        return X - (self.learning_rate*grad_X)\r\n",
    "    \r\n",
    "    def maximize(self, X, grad_X):\r\n",
    "        assert X.shape == grad_X.shape, f\"Shape mismatch, Input shape {X.shape} != Gradient shape {grad_X.shape}\"\r\n",
    "        return X + (self.learning_rate*grad_X)\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "class Sequential:\r\n",
    "    def __init__(self):\r\n",
    "        self.layers = []\r\n",
    "        self.loss = None\r\n",
    "        self.outputs = []\r\n",
    "\r\n",
    "    def add(self, layer):\r\n",
    "        self.layers.append(layer)\r\n",
    "        return self\r\n",
    "    \r\n",
    "    def summary(self):\r\n",
    "        from tabulate import tabulate\r\n",
    "\r\n",
    "        headers = [\"Layer Type\", \"Output Shape\", \"No. of parameters\"]\r\n",
    "        summary_ = []\r\n",
    "        params = 0\r\n",
    "        for layer in self.layers:\r\n",
    "            p = layer.get_no_of_params()\r\n",
    "            params += p\r\n",
    "            summary_.append([layer.__class__.__name__, layer.get_output_size(), p])\r\n",
    "        \r\n",
    "        print(tabulate(summary_, headers=headers))\r\n",
    "        print(\"Total No. of parameters:\", params)\r\n",
    "    \r\n",
    "    def fit(self, X, y, n_epochs, learning_rate, optimizer, verbose=1):\r\n",
    "        self.optimizer = optimizer.set_lr(learning_rate)\r\n",
    "\r\n",
    "        for i in range(n_epochs):\r\n",
    "            _, outputs, gradients = self.forward_propagation(X)\r\n",
    "            self.backward_propagation(outputs, gradients, y)\r\n",
    "            if verbose == 1:\r\n",
    "                print(f\"\\rEpoch: {i+1} Loss:{self._eval_loss(X, y)}\", end=\"\")\r\n",
    "        \r\n",
    "        if verbose == 0:\r\n",
    "            print(f\"\\rEpoch: {i+1} Loss:{self._eval_loss(X, y)}\", end=\"\")\r\n",
    "        \r\n",
    "        print(\"\")\r\n",
    "            \r\n",
    "    def forward_propagation(self, X):\r\n",
    "        output = X.T\r\n",
    "        outputs = [output]\r\n",
    "        gradients = []\r\n",
    "        for layer in self.layers:\r\n",
    "            grad_ = {}\r\n",
    "            grad_[\"input\"] = layer.grad_input(output)\r\n",
    "            grad_[\"w\"], grad_[\"b\"] = layer.grad_parameters(output)\r\n",
    "            output = layer.eval(output)\r\n",
    "            outputs.append(output)\r\n",
    "            gradients.append(grad_)\r\n",
    "\r\n",
    "        return output.T, outputs, gradients\r\n",
    "    \r\n",
    "    def backward_propagation(self, outputs, gradients, y):\r\n",
    "        grad_loss = self.loss.grad_input(outputs[-1], y)\r\n",
    "        outputs = outputs[:-1]\r\n",
    "        for grad, output, layer in list(zip(gradients, outputs, self.layers))[::-1]:\r\n",
    "            layer.update(grad_loss.dot(grad[\"w\"])[0], grad_loss.dot(grad[\"b\"]).T, self.optimizer)\r\n",
    "            grad_loss = grad_loss.dot(grad[\"input\"])\r\n",
    "\r\n",
    "    def _eval(self, X):\r\n",
    "        return self.forward_propagation(X)[0]\r\n",
    "    \r\n",
    "    def compile(self, loss):\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "    def _eval_loss(self, X, y_true):\r\n",
    "        if self.loss is None:\r\n",
    "            raise RuntimeError(\"Model not compiled\")\r\n",
    "            \r\n",
    "        return self.loss(self._eval(X), y_true)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(units=10, activation=Sigmoid(), input_size=10))\r\n",
    "model.add(Dense(units=10, activation=Sigmoid(), input_size=10))\r\n",
    "model.add(Dense(units=10, activation=Sigmoid(), input_size=10))\r\n",
    "model.add(Dense(units=10, activation=Sigmoid(), input_size=10))\r\n",
    "model.add(Dense(units=1, activation=Sigmoid(), input_size=10))\r\n",
    "model.compile(BinaryCrossEntropy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Layer Type    Output Shape      No. of parameters\n",
      "------------  --------------  -------------------\n",
      "Dense         (10, 1)                         110\n",
      "Dense         (10, 1)                         110\n",
      "Dense         (10, 1)                         110\n",
      "Dense         (10, 1)                         110\n",
      "Dense         (1, 1)                           11\n",
      "Total No. of parameters: 451\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "X = np.random.randn(100, 10)\r\n",
    "print(\"Y_pred\", model._eval(X))\r\n",
    "print(\"Loss\", model._eval_loss(X, np.array([1])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Y_pred [[0.97218133]]\n",
      "Loss 0.028212933625132663\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "model.fit(X, np.array([0]), n_epochs=2000, learning_rate=0.01, optimizer=GradientDescentOptimizer())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 2000 Loss:0.010992775846750647\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "model._eval(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.01093258]])"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "interpreter": {
   "hash": "952603312201d9d1df6f1b6eb4a2044a9cb2cee3bbe4f29af3f4f86c434f8702"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}