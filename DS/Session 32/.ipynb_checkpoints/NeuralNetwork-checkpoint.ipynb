{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        ix_zeros = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 0]\n",
    "        ix_ones = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 1]\n",
    "\n",
    "        y_zero = np.log(1 - y_pred[ix_zeros] + 1e-10).sum()\n",
    "        y_one = np.log(y_pred[ix_ones] + 1e-10).sum()\n",
    "\n",
    "        return -1 * (y_zero + y_one)\n",
    "    \n",
    "    def grad_input(self, X, y_true):\n",
    "        ix_zeros = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 0]\n",
    "        ix_ones = np.arange(0, y_true.shape[0])[y_true.reshape(-1) == 1]\n",
    "\n",
    "        grad = np.empty((y_true.shape[0], 1, 1), dtype=\"float\")\n",
    "        grad[ix_zeros] = 1/(1-X[:, ix_zeros].reshape(-1,1,1))\n",
    "        grad[ix_ones] = -1/X[:, ix_ones].reshape(-1,1,1)\n",
    "\n",
    "        return grad        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "    \n",
    "    def eval(self, X):\n",
    "        return 1/(1+np.e**(-1*X))\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        return np.einsum('ij,im->mij', np.identity(X.shape[0]), self.eval(X)*(1 - self.eval(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dot:\n",
    "    def __init__(self, input_size, units):\n",
    "        self.W = np.random.randn(input_size, units)\n",
    "        self.b = np.random.randn(units, 1)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.W.T.dot(X) + self.b\n",
    "\n",
    "    def grad_w(self, X):\n",
    "        I = np.identity(self.b.shape[0])\n",
    "        m1 = np.stack([I]*self.W.shape[0], axis=1)\n",
    "        grad = np.einsum('ijk,jm->mijk', m1, X)\n",
    "        return grad\n",
    "    \n",
    "    def grad_b(self, X):\n",
    "        return np.stack([np.identity(self.b.shape[0])]*X.shape[1], axis=0)\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        return np.stack([self.W.T]*X.shape[1], axis=0)\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        return self.b.shape\n",
    "    \n",
    "    def get_no_of_params(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.b.shape)\n",
    "    \n",
    "    def update(self, gradW, gradb, optimizer, method):\n",
    "        if method == \"minimize\":\n",
    "            self.W = optimizer.minimize(self.W, gradW)\n",
    "            self.b = optimizer.minimize(self.b, gradb)\n",
    "        elif method == \"maximize\":\n",
    "            self.W = optimizer.maximize(self.W, gradW)\n",
    "            self.b = optimizer.maximize(self.b, gradb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \n",
    "    def __init__(self, units, activation, input_size):\n",
    "        self.units = units\n",
    "        self.dot = Dot(input_size, units)\n",
    "        self.activation = activation\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return self.dot.get_output_size()\n",
    "\n",
    "    def get_no_of_params(self):\n",
    "        return self.dot.get_no_of_params()\n",
    "\n",
    "    def eval(self, X):\n",
    "        return self.activation(self.dot(X))\n",
    "\n",
    "    def grad_parameters(self, X):\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\n",
    "        dI_dw = self.dot.grad_w(X)\n",
    "        da_dw = np.einsum('mij,mjkl->mikl', da_dI, dI_dw)\n",
    "        \n",
    "        dI_db = self.dot.grad_b(X)\n",
    "        da_db = np.einsum('mij,mjk->mik', da_dI, dI_db)\n",
    "        return (da_dw, da_db)\n",
    "    \n",
    "    def grad_input(self, X):\n",
    "        g1 = self.activation.grad_input(self.dot(X))\n",
    "\n",
    "        g2 = self.dot.grad_input(X)\n",
    "\n",
    "        return np.einsum('mij,mjk->mik', g1, g2)\n",
    "    \n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\n",
    "        self.dot.update(grad_w, grad_b, optimizer, method)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_lr(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        return self\n",
    "    \n",
    "    def minimize(self, X, grad_X):\n",
    "        assert X.shape == grad_X.shape, f\"Shape mismatch, Input shape {X.shape} != Gradient shape {grad_X.shape}\"\n",
    "        return X - (self.learning_rate*grad_X)\n",
    "    \n",
    "    def maximize(self, X, grad_X):\n",
    "        assert X.shape == grad_X.shape, f\"Shape mismatch, Input shape {X.shape} != Gradient shape {grad_X.shape}\"\n",
    "        return X + (self.learning_rate*grad_X)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.outputs = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        return self\n",
    "    \n",
    "    def summary(self):\n",
    "        from tabulate import tabulate\n",
    "\n",
    "        headers = [\"Layer Type\", \"Output Shape\", \"No. of parameters\"]\n",
    "        summary_ = []\n",
    "        params = 0\n",
    "        for layer in self.layers:\n",
    "            p = layer.get_no_of_params()\n",
    "            params += p\n",
    "            summary_.append([layer.__class__.__name__, layer.get_output_size(), p])\n",
    "        \n",
    "        print(tabulate(summary_, headers=headers))\n",
    "        print(\"Total No. of parameters:\", params)\n",
    "    \n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        ixs = np.arange(X.shape[0])\n",
    "        np.random.shuffle(ixs)\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            x_batch, y_batch = X[ixs[i:i+batch_size]], y[ixs[i:i+batch_size]]\n",
    "            if len(x_batch):\n",
    "                yield (x_batch, y_batch)\n",
    "        return \n",
    "    \n",
    "    def fit(self, X, y, n_epochs, learning_rate, optimizer, batch_size=1, verbose=1):\n",
    "        if len(y.shape) < 2:\n",
    "            raise ValueError(f\"Incompatible shape of y {y.shape}, try reshaping y using y.reshape(-1,1)\")\n",
    "        \n",
    "        self.optimizer = optimizer.set_lr(learning_rate)\n",
    "        for i in range(n_epochs):\n",
    "            if verbose == 1:\n",
    "                print(f\"Epoch: {i+1}/{n_epochs}\")\n",
    "            \n",
    "            progress_bar = self.__progress_bar(50, int(50*batch_size/X.shape[0]))\n",
    "            for (X_batch, y_batch) in self.get_batch(X, y, batch_size):\n",
    "                time.sleep(0.01)\n",
    "                _, outputs, _gradients_ = self.forward_propagation(X_batch)\n",
    "                grads = self.backward_propagation(outputs, _gradients_, y_batch.reshape(-1,1))\n",
    "                self._update_params(grads)\n",
    "                if verbose == 1:\n",
    "                    try:\n",
    "                        _loss = self._eval_loss(X_batch, y_batch)\n",
    "                        print(\"\\r\" + next(progress_bar), f\"Loss: {np.round(_loss, 4)}\", end=\"\")\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "            if verbose == 1:\n",
    "                _loss = self._eval_loss(X, y)\n",
    "                bar =  \"|\" + \"-\"*50 + \">\" + \" \"*0 + \"|\"\n",
    "                print(\"\\r\" + bar, f\"Loss: {np.round(_loss, 4)}\")\n",
    "        if verbose == 0:\n",
    "            print(f\"\\rEpoch: {i+1} Loss:{self._eval_loss(X, y)}\", end=\"\")\n",
    "            \n",
    "        print(\"\")\n",
    "            \n",
    "    def forward_propagation(self, X, eval=False):\n",
    "        output = X.T\n",
    "        outputs = [output]\n",
    "        gradients = []\n",
    "        for layer in self.layers:\n",
    "            if not eval:\n",
    "                grad_ = {}\n",
    "                grad_[\"input\"] = layer.grad_input(output)\n",
    "                grad_[\"w\"], grad_[\"b\"] = layer.grad_parameters(output)\n",
    "                gradients.append(grad_)\n",
    "            output = layer.eval(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return output.T, outputs, gradients\n",
    "    \n",
    "    def backward_propagation(self, outputs, gradients, y):\n",
    "        grad_loss = self.loss.grad_input(outputs[-1], y)\n",
    "        outputs = outputs[:-1]\n",
    "        grads = []\n",
    "        for grad, output in list(zip(gradients, outputs))[::-1]:\n",
    "            grad_w = np.einsum('mij,mjkl->mikl', grad_loss, grad[\"w\"]).sum(axis=0)[0]\n",
    "            grad_b = np.einsum('mij,mjk->mik', grad_loss, grad[\"b\"]).sum(axis=0).T\n",
    "            grads.append((grad_w, grad_b))\n",
    "\n",
    "            grad_loss = np.einsum('mij,mjk->mik', grad_loss, grad[\"input\"])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def _update_params(self, grads):\n",
    "        for ((grad_w, grad_b), layer) in zip(grads, self.layers[::-1]):\n",
    "            layer.update(grad_w, grad_b, self.optimizer)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._eval(X)\n",
    "    \n",
    "    def predict_classes(self, X, threshold=0.5):\n",
    "        return (self.predict(X) > threshold).astype(\"int\")\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        if len(y.shape) < 2:\n",
    "            raise ValueError(f\"Incompatible shape of y {y.shape}, try reshaping y using y.reshape(-1,1)\")\n",
    "        return self._eval_loss(X, y), (y == self.predict_classes(X)).astype('int')\n",
    "    \n",
    "    def _eval(self, X):\n",
    "        return self.forward_propagation(X, eval=True)[0]\n",
    "    \n",
    "    def compile(self, loss):\n",
    "        self.loss = loss\n",
    "    \n",
    "    def __progress_bar(self, size, inc):\n",
    "        step = 0\n",
    "        inc += 1\n",
    "        while step <= size:\n",
    "\n",
    "            bar = \"|\" + \"-\"*step + \">\" + \" \"*(size-step) + \"|\"\n",
    "            yield bar\n",
    "            step += inc\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def _eval_loss(self, X, y_true):\n",
    "        if len(y_true.shape) < 2:\n",
    "            raise ValueError(f\"Incompatible shape of y {y_true.shape}, try reshaping y using y.reshape(-1,1)\")\n",
    "            \n",
    "        if self.loss is None:\n",
    "            raise RuntimeError(\"Model not compiled\")\n",
    "            \n",
    "        return self.loss(self._eval(X), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=3, activation=Sigmoid(), input_size=2))\n",
    "model.add(Dense(units=2, activation=Sigmoid(), input_size=3))\n",
    "model.add(Dense(units=1, activation=Sigmoid(), input_size=2))\n",
    "model.compile(BinaryCrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Type    Output Shape      No. of parameters\n",
      "------------  --------------  -------------------\n",
      "Dense         (3, 1)                            9\n",
      "Dense         (2, 1)                            8\n",
      "Dense         (1, 1)                            3\n",
      "Total No. of parameters: 20\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 158.0060210424048\n"
     ]
    }
   ],
   "source": [
    "X, y = make_gaussian_quantiles(n_samples=200,n_classes=2)\n",
    "y = y.reshape(-1,1)\n",
    "print(\"Loss\", model.evaluate(X, y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "|-------------------------------------------------->| Loss: 138.6248\n",
      "Epoch: 2/10\n",
      "|-------------------------------------------------->| Loss: 138.612\n",
      "Epoch: 3/10\n",
      "|-------------------------------------------------->| Loss: 138.68\n",
      "Epoch: 4/10\n",
      "|-------------------------------------------------->| Loss: 138.5882\n",
      "Epoch: 5/10\n",
      "|-------------------------------------------------->| Loss: 138.5774\n",
      "Epoch: 6/10\n",
      "|-------------------------------------------------->| Loss: 138.567Loss: 6.7688\n",
      "Epoch: 7/10\n",
      "|-------------------------------------------------->| Loss: 138.557\n",
      "Epoch: 8/10\n",
      "|-------------------------------------------------->| Loss: 138.5474\n",
      "Epoch: 9/10\n",
      "|-------------------------------------------------->| Loss: 138.5382\n",
      "Epoch: 10/10\n",
      "|-------------------------------------------------->| Loss: 138.529oss: 7.0069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, n_epochs=10, batch_size=10, learning_rate=0.001, optimizer=GradientDescentOptimizer(), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "952603312201d9d1df6f1b6eb4a2044a9cb2cee3bbe4f29af3f4f86c434f8702"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
