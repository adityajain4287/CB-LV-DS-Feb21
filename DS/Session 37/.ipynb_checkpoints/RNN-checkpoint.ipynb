{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Manu\\Desktop\\CB-LV-DS-Feb21\\DS\\NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model import Sequential\n",
    "from layer import Dense\n",
    "from loss import BinaryCrossEntropy\n",
    "from activation import Sigmoid, ReLU\n",
    "from optimizer import GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN:\n",
    "    \n",
    "    def __init__(self, output_units, hidden_units, activation, input_size):\n",
    "        if isinstance(input_size, tuple):\n",
    "            if len(input_size) != 2: # input_size => (timestep, input_features)\n",
    "                raise RuntimeError(f\"Incompatible input shape, got {input_size}\")\n",
    "                \n",
    "        self.output_activation = activation\n",
    "        self.timestep = input_size[0]\n",
    "        self.input_units = input_size[1]\n",
    "        self.output_units = output_units\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        self.hidden_layer = Dense(units=hidden_units, activation=ReLU(), input_size=self.input_units+self.hidden_units)\n",
    "        self.output_layer = Dense(units=output_units, activation=self.output_activation, input_size=self.hidden_units)\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return (self.timestep, self.output_units)\n",
    "\n",
    "    def get_no_of_params(self):\n",
    "        return self.hidden_layer.get_no_of_params() + self.output_layer.get_no_of_params()\n",
    "\n",
    "    def eval(self, X, start_sequence=None):\n",
    "        h_t = np.zeros((self.hidden_units, X.shape[-1]))\n",
    "        timestep = X.shape[1]\n",
    "        if start_sequence is not None:\n",
    "            assert h_t.shape == start_sequence, f\"Sequence start hidden state received incompatible shape, got {start_sequence.shape}, expected {h_t.shape}\"\n",
    "            h_t = start_sequence\n",
    "        \n",
    "        y = np.empty((self.output_units, timestep, X.shape[-1]))\n",
    "        for i in range(timestep):\n",
    "            x_t = X[:, i, :]\n",
    "            x_t_stacked = np.vstack([x_t, h_t])\n",
    "            h_t = self.hidden_layer.eval(x_t_stacked)\n",
    "            y_t = self.output_layer.eval(h_t)\n",
    "            y[:, i, :] = y_t\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def grad_parameters_T(self, x_t, h_t_1):\n",
    "        \n",
    "        x_t_stacked = np.vstack([x_t, h_t_1])\n",
    "        h_t = self.hidden_layer.eval(x_t_stacked)\n",
    "        \n",
    "        dyt_param_output = self.output_layer.grad_parameters(h_t) # (dw, db)\n",
    "        dyt_ht = self.output_layer.grad_input(h_t)\n",
    "        \n",
    "        dht_param = self.hidden_layer.grad_parameters(x_t_stacked)\n",
    "        \n",
    "        dyt_param_hidden = (np.einsum('mij,mjkl->mikl', dyt_ht, dht_param[0]), np.einsum('mij,mjk->mik', dyt_ht, dht_param[1]))\n",
    "        \n",
    "        return (dyt_param_output, dyt_param_hidden)\n",
    "    \n",
    "    def grad_input_T(self, x_t, h_t_1):\n",
    "        x_t_stacked = np.vstack([x_t, h_t_1])\n",
    "        h_t = self.hidden_layer.eval(x_t_stacked)\n",
    "        \n",
    "        dyt_ht = self.output_layer.grad_input(h_t)\n",
    "        \n",
    "        dht_x_t_stacked = self.hidden_layer.grad_input(x_t_stacked)\n",
    "        \n",
    "        dht_x_t = dht_x_t_stacked[:, :, :self.input_units]\n",
    "\n",
    "        dyt_x_t_stacked = np.einsum('mij,mjk->mik', dyt_ht, dht_x_t_stacked)\n",
    "\n",
    "        dyt_x_t = dyt_x_t_stacked[:, :, :self.input_units]\n",
    "        dyt_h_t_1 = dyt_x_t_stacked[:, :, self.input_units:]\n",
    "        \n",
    "        assert dyt_x_t.shape[-1] == self.input_units, f\"Shape mistmatch in input gradient for step t, {dyt_x_t.shape[-1]} != {self.input_units}\"\n",
    "        assert dyt_h_t_1.shape[-1] == self.hidden_units, f\"Shape mistmatch in input gradient for step t, {dyt_h_t_1.shape[-1]} != {self.hidden_units}\"\n",
    "        \n",
    "        return dyt_x_t, dyt_h_t_1, dht_x_t\n",
    "    \n",
    "    \n",
    "    #TBD\n",
    "    \n",
    "    def grad_input(self, X):\n",
    "        g1 = self.activation.grad_input(self.dot(X))\n",
    "\n",
    "        g2 = self.dot.grad_input(X)\n",
    "\n",
    "        return np.einsum('mij,mjk->mik', g1, g2)\n",
    "    \n",
    "    def grad_parameters(self, X):\n",
    "        \n",
    "        dyt_output = self.output_layer.grad_parameters\n",
    "        \n",
    "        da_dI = self.activation.grad_input(self.dot(X))\n",
    "        dI_dw = self.dot.grad_w(X)\n",
    "        da_dw = np.einsum('mij,mjkl->mikl', da_dI, dI_dw)\n",
    "        \n",
    "        dI_db = self.dot.grad_b(X)\n",
    "        da_db = np.einsum('mij,mjk->mik', da_dI, dI_db)\n",
    "        return (da_dw, da_db)\n",
    "    \n",
    "    def gradient_dict(self, output):\n",
    "        grad_ = {}\n",
    "        grad_[\"input\"] = self.grad_input(output)\n",
    "        grad_[\"w\"], grad_[\"b\"] = self.grad_parameters(output)\n",
    "\n",
    "        return grad_\n",
    "    \n",
    "    @staticmethod\n",
    "    def backprop_grad(grad_loss, grad):\n",
    "        grad_w = np.einsum('mij,mjkl->mikl', grad_loss, grad[\"w\"]).sum(axis=0)[0]\n",
    "        grad_b = np.einsum('mij,mjk->mik', grad_loss, grad[\"b\"]).sum(axis=0).T\n",
    "        grad_loss = np.einsum('mij,mjk->mik', grad_loss, grad[\"input\"])\n",
    "\n",
    "        return grad_w, grad_b, grad_loss\n",
    "\n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\n",
    "        self.dot.update(grad_w, grad_b, optimizer, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10, 5, 1).T[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10, 5, 1).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = BasicRNN(output_units=3, hidden_units=5, activation=Sigmoid(), input_size=(10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99370971],\n",
       "       [0.29087072],\n",
       "       [0.01155823]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.eval(np.random.randn(1, 20, 2).T)[:, 4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
