{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \n",
    "    def __init__(self, ksize, stride, padding, activation, filters, input_size):\n",
    "        self.kernels = []\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_size = input_size\n",
    "        self.ksize = ksize\n",
    "        self.filters = filters\n",
    "        \n",
    "        self.bias = np.random.randn(filters).reshape(1, -1)\n",
    "        for i in range(filters):\n",
    "            self.kernels.append(np.random.randn((ksize, ksize, input_size[-1])))\n",
    "        self.activation = activation\n",
    "        \n",
    "    @staticmethod\n",
    "    def _rotate(inp):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "\n",
    "        return np.flip(inp, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _inside_pad(inp, pad_width):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        \n",
    "        if pad_width == 0:\n",
    "            return inp\n",
    "        ix = np.repeat(np.arange(1, arr.shape[1]), pad_width)\n",
    "\n",
    "        inp = np.insert(inp, ix, 0, axis=1)\n",
    "        return  np.insert(inp, ix, 0, axis=2)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad(inp, pad_width):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        if pad_width == 0:\n",
    "            return inp\n",
    "\n",
    "        return np.pad(arr, [(0, 0), (pad_width, pad_width), (pad_width, pad_width), (0,0)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _convolution_op_w_kernel(inp, kernel, stride=1):\n",
    "        \n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        assert len(kernel.shape) == 4, f\"Shape mismatch, kernel should have 4 dim, got {len(kernel.shape)}\"\n",
    "        assert inp.shape[-1] == kernel.shape[-1], f\"Shape mismatch, input map should have same channel as kernel, got {inp.shape[-1]} & {kernel.shape[-1]}\"\n",
    "        assert kernel.shape[0] == kernel.shape[1], \"Non square kernels are not supported\"\n",
    "        assert (inp.shape[1] >= kernel.shape[0]) or (inp.shape[2] >= kernel.shape[1]), f\"Input map shape less than kernel, got {inp.shape[1:3]} for kernel {kernel.shape[:-1]}\"\n",
    "\n",
    "        kernel = self._rotate(kernel)\n",
    "\n",
    "        start_rloc = 0\n",
    "        end_rloc = kernel.shape[1]\n",
    "\n",
    "        oup = []\n",
    "\n",
    "        while end_rloc <= inp.shape[1]:\n",
    "\n",
    "            start_cloc = 0\n",
    "            end_cloc = kernel.shape[2]\n",
    "            output = []\n",
    "\n",
    "            while end_cloc <= inp.shape[2]:\n",
    "                conv = inp[:, start_rloc:end_rloc, start_cloc:end_cloc]*kernel\n",
    "                output.append(conv.sum(axis=(1,2,3)))\n",
    "\n",
    "                start_cloc += stride\n",
    "                end_cloc += stride\n",
    "\n",
    "            oup.append(output)\n",
    "\n",
    "            start_rloc += stride\n",
    "            end_rloc += stride        \n",
    "            \n",
    "        return np.moveaxis(oup, -1, 0)\n",
    "    \n",
    "    def _convolution_op(self, inp, stride): \n",
    "        \n",
    "        feature_maps = []\n",
    "        for kernel in self.kernels:\n",
    "            oup = self._convolution_op_w_kernel(inp, np.expand_dims(kernel, 0), stride)\n",
    "            feature_maps.append(oup)\n",
    "        \n",
    "        return np.stack(feature_maps, axis=-1)\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        m, n, k, p, s = self.input_size[0], self.input_size[1], self.ksize, self.padding, self.stride\n",
    "        return (m-k+2*p)//s + 1, (n-k+2*p)//s + 1, self.filters\n",
    "\n",
    "    def get_no_of_params(self):\n",
    "        return (self.ksize*self.ksize*self.input_size[-1]*self.filters) + self.filters\n",
    "\n",
    "    def eval(self, X):\n",
    "        out = self._convolution_op(X.T, self.stride) + self.bias\n",
    "        b, h, w, c = out.shape\n",
    "        a_out = self.activation(out.reshape(b, h*w*c).T)\n",
    "        \n",
    "        return a_out.T.reshape(b, h, w, c)\n",
    "\n",
    "    def grad_parameters(self, X):\n",
    "        out = self._convolution_op(X.T, self.stride) + self.bias\n",
    "        b, h, w, c = out.shape\n",
    "        \n",
    "        da_dI = self.activation.grad_input(out.reshape(b, h*w*c).T)\n",
    "        da_dI = np.diagonal(da_dI, axis1=1, axis2=2)\n",
    "        \n",
    "        return da_dI.T.reshape(b, h, w, c), None\n",
    "    \n",
    "    def grad_input(self, X):\n",
    "        \n",
    "        return X.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def backprop_grad(abcd, grad): # abcd -> grad_loss\n",
    "        pqrs = grad[\"w\"]\n",
    "        \n",
    "        kernels = pqrs * abcd\n",
    "        kernels = self._inside_pad(kernels, self.stride-1)\n",
    "        inps = grad[\"input\"]\n",
    "        grad_ws = []\n",
    "        \n",
    "        #### GRAD W---------------------------\n",
    "        for i in range(kernels.shape[-1]):\n",
    "            kernel = kernels[..., i]\n",
    "            grad_w = []\n",
    "            for j in range(inps.shape[-1]):\n",
    "                inp = inps[..., i]\n",
    "                oup = self._convolution_op_w_kernel(inp, kernel)\n",
    "                oup = self._rotate(oup).sum(axis=0)\n",
    "                grad_w.append(oup)\n",
    "            grad_ws.append(np.array(grad_w))\n",
    "        ### -----------------------------------\n",
    "        \n",
    "        #### GRAD I---------------------------\n",
    "        inp = self._pad(kernels, self.ksize-1)\n",
    "        kernels = self.kernels\n",
    "        grad_I = np.empty(grad[\"input\"].shape, dtype=\"float\")\n",
    "        \n",
    "        for i in range(self.input_size[-1]):\n",
    "            kernel = np.dstack([kernels[j][...,i] for j in range(len(kernels))])\n",
    "            oup = self._convolution_op_w_kernel(inp, kernel)\n",
    "            grad_I[..., i] = oup\n",
    "        ### -----------------------------------\n",
    "        \n",
    "        #### GRAD b---------------------------\n",
    "        grad_bs = np.sum(pqrs * abcd, axis=(1,2,0))\n",
    "\n",
    "        return grad_ws, grad_bs, grad_I\n",
    "\n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool, Flatten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
