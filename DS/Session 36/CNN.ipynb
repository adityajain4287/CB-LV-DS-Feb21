{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \n",
    "    def __init__(self, ksize, stride, padding, activation, filters, input_size):\n",
    "        if input_size[0] <= 0 or input_size[1] <= 0:\n",
    "            raise ValueError(f\"Input image size is invalid, got {input_size}\")\n",
    "        self.kernels = []\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_size = input_size\n",
    "        self.ksize = ksize\n",
    "        self.filters = filters\n",
    "        \n",
    "        self.bias = np.random.randn(filters).reshape(1, -1)\n",
    "        for i in range(filters):\n",
    "            self.kernels.append(np.random.randn(ksize, ksize, input_size[-1]))\n",
    "        self.activation = activation\n",
    "        \n",
    "    @staticmethod\n",
    "    def _rotate(inp):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "\n",
    "        return np.flip(inp, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _inside_pad(inp, pad_width):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        \n",
    "        if pad_width == 0:\n",
    "            return inp\n",
    "        ix = np.repeat(np.arange(1, inp.shape[1]), pad_width)\n",
    "\n",
    "        inp = np.insert(inp, ix, 0, axis=1)\n",
    "        return  np.insert(inp, ix, 0, axis=2)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad(inp, pad_width):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        if pad_width == 0:\n",
    "            return inp\n",
    "\n",
    "        return np.pad(inp, [(0, 0), (pad_width, pad_width), (pad_width, pad_width), (0,0)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _convolution_op_w_kernel(inp, kernel, stride=1):\n",
    "        \n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        assert len(kernel.shape) == 4, f\"Shape mismatch, kernel should have 4 dim, got {len(kernel.shape)}\"\n",
    "        assert inp.shape[-1] == kernel.shape[-1], f\"Shape mismatch, input map should have same channel as kernel, got {inp.shape} & {kernel.shape}\"\n",
    "        assert kernel.shape[1] == kernel.shape[2], \"Non square kernels are not supported\"\n",
    "        assert (inp.shape[1] >= kernel.shape[0]) or (inp.shape[2] >= kernel.shape[1]), f\"Input map shape less than kernel, got {inp.shape[1:3]} for kernel {kernel.shape[:-1]}\"\n",
    "\n",
    "        kernel = Conv2D._rotate(kernel)\n",
    "\n",
    "        start_rloc = 0\n",
    "        end_rloc = kernel.shape[1]\n",
    "\n",
    "        oup = []\n",
    "\n",
    "        while end_rloc <= inp.shape[1]:\n",
    "\n",
    "            start_cloc = 0\n",
    "            end_cloc = kernel.shape[2]\n",
    "            output = []\n",
    "\n",
    "            while end_cloc <= inp.shape[2]:\n",
    "                conv = inp[:, start_rloc:end_rloc, start_cloc:end_cloc]*kernel\n",
    "                output.append(conv.sum(axis=(1,2,3)))\n",
    "\n",
    "                start_cloc += stride\n",
    "                end_cloc += stride\n",
    "\n",
    "            oup.append(output)\n",
    "\n",
    "            start_rloc += stride\n",
    "            end_rloc += stride        \n",
    "        \n",
    "        oup = np.expand_dims(oup, 0)\n",
    "        oup = np.transpose(oup, [3,1,2,0])\n",
    "        assert len(oup.shape) == 4, f\"Shape mismatch at convolution op, got {oup.shape}\"\n",
    "        \n",
    "        return oup\n",
    "    \n",
    "    def _convolution_op(self, inp, stride): \n",
    "        \n",
    "        feature_maps = []\n",
    "        for kernel in self.kernels:\n",
    "            oup = self._convolution_op_w_kernel(inp, np.expand_dims(kernel, 0), stride)\n",
    "            feature_maps.append(oup[...,0])\n",
    "        \n",
    "        return np.stack(feature_maps, axis=-1)\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        m, n, k, p, s = self.input_size[0], self.input_size[1], self.ksize, self.padding, self.stride\n",
    "        return (m-k+2*p)//s + 1, (n-k+2*p)//s + 1, self.filters\n",
    "\n",
    "    def get_no_of_params(self):\n",
    "        return (self.ksize*self.ksize*self.input_size[-1]*self.filters) + self.filters\n",
    "\n",
    "    def eval(self, X):\n",
    "        out = self._convolution_op(X.T, self.stride) + self.bias\n",
    "        b, h, w, c = out.shape\n",
    "        a_out = self.activation(out.reshape(b, h*w*c).T)\n",
    "        return a_out.T.reshape(b, h, w, c).T\n",
    "\n",
    "    def grad_activation(self, X):\n",
    "        out = self._convolution_op(X.T, self.stride) + self.bias\n",
    "        b, h, w, c = out.shape\n",
    "        \n",
    "        da_dI = self.activation.grad_input(out.reshape(b, h*w*c).T)\n",
    "        da_dI = np.diagonal(da_dI, axis1=1, axis2=2)\n",
    "        \n",
    "        return da_dI.T.reshape(b, h, w, c)\n",
    "    \n",
    "        \n",
    "    def gradient_dict(self, output):\n",
    "        grad_ = {}\n",
    "        grad_[\"input\"] = self.get_input(output)\n",
    "        grad_[\"activation\"] = self.grad_activation(output)\n",
    "\n",
    "        return grad_\n",
    "    \n",
    "    def get_input(self, X):\n",
    "        out_h, out_w, _ = self.get_output_size()\n",
    "        h = (out_h-1)*self.stride-2*self.padding+self.ksize\n",
    "        w = (out_w-1)*self.stride-2*self.padding+self.ksize\n",
    "        return X.T[:, :h, :w, :]\n",
    "    \n",
    "    def backprop_grad(self, abcd, grad): # abcd -> grad_loss\n",
    "        pqrs = grad[\"activation\"]\n",
    "        \n",
    "        b, h, w, c = abcd.shape\n",
    "        \n",
    "        kernels = pqrs[:, :h, :w, :] * abcd\n",
    "        kernels = self._inside_pad(kernels, self.stride-1)\n",
    "        inps = grad[\"input\"]\n",
    "        grad_ws = []\n",
    "        \n",
    "        #### GRAD W---------------------------\n",
    "        for i in range(kernels.shape[-1]):\n",
    "            kernel = kernels[..., i]\n",
    "            grad_w = []\n",
    "            for j in range(inps.shape[-1]):\n",
    "                inp = inps[..., j]\n",
    "                oup = self._convolution_op_w_kernel(np.expand_dims(inp,-1), np.expand_dims(kernel, -1))\n",
    "                oup = self._rotate(oup).sum(axis=0)\n",
    "                grad_w.append(oup[...,0])\n",
    "            \n",
    "            grad_w = np.array(grad_w)\n",
    "            grad_ws.append(np.transpose(grad_w, [1,2,0]))\n",
    "        ### -----------------------------------\n",
    "        \n",
    "        #### GRAD I---------------------------\n",
    "        inp = self._pad(kernels, self.ksize-1)\n",
    "        kernels = self.kernels\n",
    "        \n",
    "        grad_I = np.empty_like(grad[\"input\"], dtype=\"float32\")\n",
    "        \n",
    "        for i in range(self.input_size[-1]):\n",
    "            kernel = np.dstack([kernels[j][...,i] for j in range(len(kernels))])\n",
    "            oup = self._convolution_op_w_kernel(inp, np.expand_dims(kernel, 0))\n",
    "            grad_I[..., i] = oup[...,0]\n",
    "            \n",
    "        ### -----------------------------------\n",
    "        \n",
    "        #### GRAD b---------------------------\n",
    "        grad_bs = np.sum(pqrs * abcd, axis=(1,2,0))\n",
    "        \n",
    "        return grad_ws, grad_bs.reshape(1,-1), self._pad_grad_I(grad_I)\n",
    "\n",
    "    def _pad_grad_I(self, grad_I):\n",
    "        return np.pad(grad_I, [(0, 0), (0, self.input_size[0] - grad_I.shape[1]), (0, self.input_size[1] - grad_I.shape[2]), (0,0)])\n",
    "        \n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\n",
    "        if method==\"minimize\":\n",
    "            self.bias = optimizer.minimize(self.bias, grad_b)\n",
    "            for i in range(len(self.kernels)):\n",
    "                self.kernels[i] = optimizer.minimize(self.kernels[i], grad_w[i])\n",
    "        else:\n",
    "            self.bias = optimizer.maximize(self.bias, grad_b)\n",
    "            for i in range(len(self.kernels)):\n",
    "                self.kernels[i] = optimizer.maximize(self.kernels[i], grad_w[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self, input_size):\n",
    "        if input_size[0] <= 0 or input_size[1] <= 0:\n",
    "            raise ValueError(f\"Input image size is invalid, got {input_size}\")\n",
    "        \n",
    "        self.h, self.w, self.c = input_size\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return (self.h*self.w*self.c, 1)\n",
    "\n",
    "    def get_no_of_params(self):\n",
    "        return 0\n",
    "\n",
    "    def eval(self, X):\n",
    "        return X.T.reshape(-1, self.h*self.w*self.c).T\n",
    "\n",
    "    def grad_parameters(self, X):\n",
    "        pass\n",
    "    \n",
    "    def gradient_dict(self, output):\n",
    "        grad_ = {}\n",
    "        return grad_\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        pass\n",
    "    \n",
    "    def backprop_grad(self, grad_loss, grad):\n",
    "        # m x 1 x self.h*self.w*self.c\n",
    "        return None, None, grad_loss[:, 0, :].reshape(-1, self.h, self.w, self.c)\n",
    "\n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    \n",
    "    def __init__(self, ksize, stride, padding, input_size):\n",
    "        if input_size[0] <= 0 or input_size[1] <= 0:\n",
    "            raise ValueError(f\"Input image size is invalid, got {input_size}\")\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_size = input_size\n",
    "        self.ksize = ksize\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad(inp, pad_width):\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "        if pad_width == 0:\n",
    "            return inp\n",
    "\n",
    "        return np.pad(inp, [(0, 0), (pad_width, pad_width), (pad_width, pad_width), (0,0)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _convolution_op_w_kernel(inp, ksize, stride=1):\n",
    "\n",
    "        assert len(inp.shape) == 4, f\"Shape mismatch, input map should have 4 dim, got {len(inp.shape)}\"\n",
    "\n",
    "        start_rloc = 0\n",
    "        end_rloc = ksize\n",
    "\n",
    "        oup = []\n",
    "        ix = []\n",
    "        inp_ixs = np.dstack(np.meshgrid(np.arange(inp.shape[1]), np.arange(inp.shape[2]))[::-1])\n",
    "        inp_ixs = np.repeat(np.expand_dims(inp_ixs, 0), inp.shape[0], axis=0)\n",
    "\n",
    "        while end_rloc <= inp.shape[1]:\n",
    "\n",
    "            start_cloc = 0\n",
    "            end_cloc = ksize\n",
    "            output = []\n",
    "            indexes = []\n",
    "\n",
    "            while end_cloc <= inp.shape[2]:\n",
    "                conv = inp[:, start_rloc:end_rloc, start_cloc:end_cloc]\n",
    "                ixs = inp_ixs[:, start_rloc:end_rloc, start_cloc:end_cloc]\n",
    "\n",
    "                b, h, w, c = conv.shape\n",
    "                conv = conv.reshape(b, h*w, c)\n",
    "                output.append(conv.max(axis=1))\n",
    "                indexes.append(ixs.reshape(b, h*w, 2)[np.arange(b).reshape(-1,1), conv.argmax(axis=1)])\n",
    "\n",
    "                start_cloc += stride\n",
    "                end_cloc += stride\n",
    "\n",
    "            oup.append(output)\n",
    "            ix.append(indexes)\n",
    "\n",
    "            start_rloc += stride\n",
    "            end_rloc += stride        \n",
    "\n",
    "        oup = np.transpose(oup, [2,0,1,3])\n",
    "        ix = np.transpose(ix, [2,0,1,3, 4])\n",
    "        assert len(oup.shape) == 4, f\"Shape mismatch at convolution op, got {oup.shape}\"\n",
    "        assert len(ix.shape) == 5, f\"Shape mismatch at convolution op, got {ix.shape}\"\n",
    "\n",
    "        return oup, ix\n",
    "    \n",
    "    def _convolution_op(self, inp, stride): \n",
    "\n",
    "        oup, ix = self._convolution_op_w_kernel(inp, self.ksize, stride)\n",
    "        \n",
    "        return oup, ix\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        m, n, k, p, s = self.input_size[0], self.input_size[1], self.ksize, self.padding, self.stride\n",
    "        return (m-k+2*p)//s + 1, (n-k+2*p)//s + 1, self.input_size[-1]\n",
    "\n",
    "    def get_no_of_params(self):\n",
    "        return 0\n",
    "\n",
    "    def eval(self, X, eval=True):\n",
    "        out, ix = self._convolution_op(X.T, self.stride)\n",
    "        return out.T if eval else (out.T, ix)\n",
    "\n",
    "    def grad_activation(self, X):\n",
    "        pass\n",
    "        \n",
    "    def gradient_dict(self, X):\n",
    "        grad_ = {}\n",
    "        _, grad_[\"max_indexes\"] = self.eval(X, False)\n",
    "        grad_[\"input\"] = self.get_input(X)\n",
    "\n",
    "        return grad_\n",
    "    \n",
    "    def get_input(self, X):\n",
    "        out_h, out_w, _ = self.get_output_size()\n",
    "        h = (out_h-1)*self.stride-2*self.padding+self.ksize\n",
    "        w = (out_w-1)*self.stride-2*self.padding+self.ksize\n",
    "        return np.zeros_like(X.T[:, :h, :w, :])\n",
    "    \n",
    "    def backprop_grad(self, abcd, grad): # abcd -> grad_loss\n",
    "        grad_I = grad[\"input\"]\n",
    "        max_indexes = grad[\"max_indexes\"]\n",
    "        b, h, w, c = grad_I.shape\n",
    "        \n",
    "        #### GRAD I---------------------------\n",
    "        for i in range(abcd.shape[1]):\n",
    "            for j in range(abcd.shape[2]):\n",
    "                grad_I[np.arange(b).reshape(-1,1), max_indexes[:, i, j, :, 0], max_indexes[:, i, j, :, 1], np.arange(c).reshape(1,-1)] += abcd[:, i, j, :]\n",
    "        ### -----------------------------------\n",
    "\n",
    "        return None, None, self._pad_grad_I(grad_I)\n",
    "\n",
    "    def _pad_grad_I(self, grad_I):\n",
    "        return np.pad(grad_I, [(0, 0), (0, self.input_size[0] - grad_I.shape[1]), (0, self.input_size[1] - grad_I.shape[2]), (0,0)])\n",
    "    \n",
    "    def update(self, grad_w, grad_b, optimizer, method=\"minimize\"):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Manu\\Desktop\\CB-LV-DS-Feb21\\DS\\NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Sequential\n",
    "from layer import Dense\n",
    "from loss import BinaryCrossEntropy\n",
    "from activation import Sigmoid\n",
    "from optimizer import GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.vstack([x_train[y_train == 0][:500], x_train[y_train == 1][:500]]).reshape(-1,28,28,1)/255\n",
    "y_data = np.hstack([y_train[y_train == 0][:500], y_train[y_train == 1][:500]]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Type    Output Shape      No. of parameters\n",
      "------------  --------------  -------------------\n",
      "Conv2D        (12, 12, 20)                    520\n",
      "MaxPool2D     (10, 10, 20)                      0\n",
      "Conv2D        (8, 8, 15)                     2715\n",
      "Conv2D        (6, 6, 10)                     1360\n",
      "Flatten       (360, 1)                          0\n",
      "Dense         (1, 1)                          361\n",
      "Total No. of parameters: 4956\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D, ksize=5, stride=2, activation=Sigmoid(), input_size=(28,28,1), filters=20, padding=0)\n",
    "model.add(MaxPool2D, ksize=3, stride=1, padding=0)\n",
    "model.add(Conv2D, ksize=3, stride=1, activation=Sigmoid(), filters=15, padding=0)\n",
    "model.add(Conv2D, ksize=3, stride=1, activation=Sigmoid(), filters=10, padding=0)\n",
    "model.add(Flatten)\n",
    "model.add(Dense, units=1, activation=Sigmoid())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(BinaryCrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.035084596994722"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._eval_loss(x_data[:10], y_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "|-------------------------------------------------->| Loss: 3.61459"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-872-1df906d63fe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0003\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\CB-LV-DS-Feb21\\DS\\NN\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, n_epochs, learning_rate, optimizer, batch_size, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mprogress_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__progress_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_gradients_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_gradients_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\CB-LV-DS-Feb21\\DS\\NN\\model.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self, X, eval)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mgrad_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                 \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-689-1492a1542d00>\u001b[0m in \u001b[0;36mgradient_dict\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mgrad_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mgrad_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mgrad_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"activation\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_data, y_data, n_epochs=1, batch_size=10, learning_rate=0.0003, optimizer=GradientDescentOptimizer(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.177907545943706"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._eval_loss(x_data[:10], y_data[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
